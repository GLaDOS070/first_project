{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd7d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import lxml\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4863ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —É–∫–∞–∑–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "user = 'MerinovDV'\n",
    "path_to_credential = f'C:/Users/{user}/Downloads/auto-monitoring-367212-64ec4ad9d3a5.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89872bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ Google Spreadsheets\n",
    "\n",
    "# Specify path to your file with credentials\n",
    "path_to_credential = path_to_credential \n",
    "\n",
    "# Specify name of table in google sheets\n",
    "table_name = '–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–∑—ã hh2022'\n",
    "\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(path_to_credential, scope)\n",
    "\n",
    "gs = gspread.authorize(credentials)\n",
    "work_table = gs.open(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036638cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url, folder, user, add_folder=None, prof_obl=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ—Ö–æ–¥–∏—Ç —Å –ø–æ–º–æ—â—å—é selenium –ø–æ –≤—Å–µ–º —Å—Å—ã–ª–∫–∞–º, —Å—á–∏—Ç–∞–µ—Ç –∫–æ–ª-–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —ç—Ç–æ–≥–æ —Å–æ–±–∏—Ä–∞–µ—Ç\n",
    "    –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –ø–∞–ø–∫–∏\n",
    "    \n",
    "    url - str, —Å—Å—ã–ª–∫–∞ —Å –∑–∞–ø—Ä–æ—Å–æ–º\n",
    "    folder - str, –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏\n",
    "    prof_obl - bool–± –µ—Å–ª–∏ –ø–∞—Ä—Å–∏–º –∏–∑ –≤–∫–ª–∞–¥–∫–∏ –ø—Ä–æ—Ñ–æ–±–ª–∞—Å—Ç–∏ - True\n",
    "    user - str, —Ç–µ–∫—É—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å\n",
    "    \"\"\"\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    s = Service(executable_path=f\"C:/Users/{user}/Parsing_folder/chromedriver/chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=s, options=options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        try:\n",
    "            pages = int(soup.find_all(attrs={'class':'bloko-button','rel': 'nofollow', 'data-qa':'pager-page'})[-1].getText())\n",
    "        except:\n",
    "            pages = 1\n",
    "        for page in range(pages):\n",
    "            if prof_obl:\n",
    "                driver.get(url+ '?page=' + str(page))\n",
    "                time.sleep(0.25)\n",
    "            else:\n",
    "                driver.get(url+ '&page=' + str(page))\n",
    "                time.sleep(0.25)\n",
    "            \n",
    "            if add_folder is not None:\n",
    "                with open(f\"C:/Users/{user}/hh/{add_folder}/{folder}/page_{page}_{date.today()}.html\", \n",
    "                            \"w\", encoding='utf-8') as file:\n",
    "                    file.write(driver.page_source)\n",
    "            else:\n",
    "                with open(f\"C:/Users/{user}/hh/{folder}/page_{page}_{date.today()}.html\", \n",
    "                            \"w\", encoding='utf-8') as file:\n",
    "                    file.write(driver.page_source)\n",
    "    except Exception as ex:\n",
    "                print(ex)\n",
    "\n",
    "    finally:\n",
    "        driver.close()\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73ea1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_vacancy(folder, user, add_folder=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    –≤–æ–≤–∑—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏\n",
    "    folder - str, –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏\n",
    "    user - str, —Ç–µ–∫—É—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å\n",
    "    \"\"\"\n",
    "    \n",
    "    links_list = []\n",
    "    if add_folder is None:\n",
    "        path = f\"C:/Users/{user}/hh/{folder}\"\n",
    "    else:\n",
    "        path = f\"C:/Users/{user}/hh/{add_folder}/{folder}\"\n",
    "    for html in os.listdir(path):\n",
    "        with open(f\"{path}/{html}\", encoding='utf-8') as file:\n",
    "            src = file.read()\n",
    "\n",
    "            try:\n",
    "                soup = BeautifulSoup(src, \"lxml\")\n",
    "                result = soup.find_all(\"a\", class_=\"serp-item__title\")\n",
    "                if len(result) != 0:\n",
    "                    for i in range(len(result)):\n",
    "                        link = result[i].get('href')\n",
    "                        links_list.append(link)\n",
    "                else:\n",
    "                    print(f'[INFO_PARSER] –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º {html}')\n",
    "\n",
    "            except:\n",
    "                print('[INFO] –ø—Ä–æ–±–ª–µ–º–∞ —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º soup')  \n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93091de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vacancy_info(dict_with_links_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏\n",
    "    dict_with_links_list - dict, —Å–ª–æ–≤–∞—Ä—å, –≤ –∫–æ—Ç–æ—Ä–æ–º –∫–ª—é—á - –ø–∞–ø–∫–∞, –∞ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∫–ª—é—á—É\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"}\n",
    "    \n",
    "    dict_with_df = {}\n",
    "    \n",
    "    for key in dict_with_links_list:\n",
    "        name = []\n",
    "        salary_from = []\n",
    "        salary_to = []\n",
    "        salary_currency = []\n",
    "        salary_gross = []\n",
    "        experience_name = []\n",
    "        schedule_name = []\n",
    "        employment_name = []\n",
    "        description = []\n",
    "        key_skills = []\n",
    "        employer = []\n",
    "        published_at = []\n",
    "        url = []\n",
    "        views = []\n",
    "        \n",
    "        for link in tqdm(dict_with_links_list[key]):\n",
    "            flag = False\n",
    "            retry = 0\n",
    "            while flag == False:\n",
    "                r = requests.get(link, headers=headers).text\n",
    "                soup = BeautifulSoup(r, 'lxml')\n",
    "                if soup.find(attrs={'data-qa': 'vacancy-title'}) is not None:\n",
    "                    \n",
    "                    # –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏\n",
    "                    title = soup.find(attrs={'data-qa': 'vacancy-title'})\n",
    "                    if title is not None:\n",
    "                        name.append(title.getText())\n",
    "                    else:\n",
    "                        name.append('null')\n",
    "\n",
    "                    # –∑–∞—Ä–ø–ª–∞—Ç–∞\n",
    "                    salary = soup.find(attrs={'data-qa': 'vacancy-salary'})\n",
    "                    if salary is not None:\n",
    "                        salary = salary.getText().replace(u'\\xa0', u'').split(' ')\n",
    "                        if '–æ—Ç' in salary:\n",
    "                            salary_from_i = salary.index('–æ—Ç') + 1\n",
    "                            salary_from.append(salary[salary_from_i])\n",
    "                        else:\n",
    "                            salary_from.append('null')\n",
    "                        if '–¥–æ' in salary:\n",
    "                            salary_to_i = salary.index('–¥–æ') + 1\n",
    "                            if salary[salary_to_i] == '–≤—ã—á–µ—Ç–∞':\n",
    "                                salary_to.append('null')\n",
    "                            else:\n",
    "                                salary_to.append(salary[salary_to_i])\n",
    "                        else:\n",
    "                            salary_to.append('null')\n",
    "                    else:\n",
    "                        salary_from.append('null')\n",
    "                        salary_to.append('null')\n",
    "\n",
    "                    # –≤–∞–ª—é—Ç–∞\n",
    "                    sal_currency = soup.find(attrs={'data-qa': 'vacancy-salary'})\n",
    "                    if sal_currency is not None:\n",
    "                        sal_currency = sal_currency.getText().split(' ')\n",
    "                        if '—Ä—É–±.' in sal_currency:\n",
    "                            currency = 'RUB'\n",
    "                        elif 'USD' in sal_currency:\n",
    "                            currency = 'USD'\n",
    "                        elif 'EUR' in sal_currency:\n",
    "                            currency = 'EUR'\n",
    "                        elif 'KZT' in sal_currency:\n",
    "                            currency = 'KZT'\n",
    "                        elif '–±–µ–ª. —Ä—É–±' in sal_currency:\n",
    "                            currency = 'BYN'\n",
    "                        elif 'KGS' in sal_currency:\n",
    "                            currency = 'KGS'\n",
    "                        elif '—Å—É–º' in sal_currency:\n",
    "                            currency = 'UZS'\n",
    "                        elif 'AZN' in sal_currency:\n",
    "                            currency = 'AZN'\n",
    "                        else:\n",
    "                            currency = 'null'\n",
    "                        salary_currency.append(currency)\n",
    "                    else:\n",
    "                        salary_currency.append('null')\n",
    "\n",
    "                    # —Ç–∏–ø –∑–∞—Ä–ø–ª–∞—Ç—ã\n",
    "                    salary_type = soup.find('span', attrs={'class': 'vacancy-salary-compensation-type'})\n",
    "                    if salary_type is not None:\n",
    "                        salary_gross.append(salary_type.getText().strip())\n",
    "                    else: \n",
    "                            salary_gross.append('null')\n",
    "\n",
    "                    # experience_name\n",
    "                    experience = soup.find(attrs={'data-qa': 'vacancy-experience'})\n",
    "                    if experience is not None:\n",
    "                        experience_name.append(experience.getText())\n",
    "                    else: \n",
    "                        experience_name.append('null')\n",
    "\n",
    "                    # employment_name, schedule_name\n",
    "                    employment_schedule = soup.find(attrs={'data-qa': 'vacancy-view-employment-mode'})\n",
    "                    if employment_schedule is not None:\n",
    "                        employment_name.append(employment_schedule.getText().split(', ')[0])\n",
    "                        schedule_name.append(employment_schedule.getText().split(', ')[1])\n",
    "                    else: \n",
    "                        employment_name.append('null')\n",
    "                        schedule_name.append('null')\n",
    "\n",
    "                    # –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏\n",
    "                    descrip = soup.find('div', class_='vacancy-section')\n",
    "                    if descrip is not None:\n",
    "                        description.append(descrip.getText())\n",
    "                    else:\n",
    "                        description.append('null')\n",
    "\n",
    "                    # key_skills\n",
    "                    key_skills_i = []\n",
    "                    skills = soup.find_all(attrs={'class': 'bloko-tag bloko-tag_inline'})\n",
    "                    if len(skills) != 0:\n",
    "                        for i in soup.find_all(attrs={'class': 'bloko-tag bloko-tag_inline'}):\n",
    "                            key_skills_i.append(i.getText())\n",
    "                        key_skills.append(key_skills_i)\n",
    "                    else:\n",
    "                        key_skills.append('null')\n",
    "\n",
    "                    # –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏\n",
    "                    emp = soup.find('span', class_='vacancy-company-name')\n",
    "                    if emp is not None:\n",
    "                        employer.append(emp.getText())\n",
    "                    else:\n",
    "                        employer.append('null')\n",
    "\n",
    "                    # –∫–æ–ª-–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å–µ–π—á–∞—Å\n",
    "                    view = soup.find('span', class_='vacancy-viewers-count')\n",
    "                    if view is not None:\n",
    "                        views.append(view.getText().replace(u'\\xa0', u' ').split(' ')[0])\n",
    "                    else:\n",
    "                        views.append('null')\n",
    "\n",
    "                    # –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞\n",
    "                    dt = soup.find('p', class_='vacancy-creation-time-redesigned')\n",
    "                    if dt is not None:\n",
    "                        published_at.append(dt.getText().replace(u'\\xa0', u' ')\\\n",
    "                                                    .split('–æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞')[1]\\\n",
    "                                                    .split('–≤')[0].strip())\n",
    "                    else:\n",
    "                        published_at.append('null')\n",
    "\n",
    "                    url.append(link)\n",
    "\n",
    "                    time.sleep(0.25)\n",
    "\n",
    "                    \n",
    "                    flag = True\n",
    "\n",
    "                    \n",
    "                retry += 1\n",
    "                if retry == 5:\n",
    "                    break\n",
    "\n",
    "        output = pd.DataFrame()\n",
    "        output['name'] = name\n",
    "        output['salary_from'] = salary_from\n",
    "        output['salary_to'] = salary_to\n",
    "        output['salary_currency'] = salary_currency\n",
    "        output['salary_gross'] = salary_gross\n",
    "        output['experience_name'] = experience_name\n",
    "        output['schedule_name'] = schedule_name\n",
    "        output['employment_name'] = employment_name\n",
    "        output['description'] = description\n",
    "        output['key_skills'] = key_skills\n",
    "        output['employer'] = employer\n",
    "        output['published_at'] = published_at\n",
    "        output['url'] = url\n",
    "        output['views'] = views\n",
    "        output['type'] = key\n",
    "        \n",
    "        dict_with_df[key] = output\n",
    "\n",
    "    return dict_with_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48db2ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Å –∫–æ–ª-–≤–æ–º –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞–≤—ã–∫–æ–≤\n",
    "def count_skills(column, to_df=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å: –∫–ª—é—á - –Ω–∞–≤—ã–∫, –∑–Ω–∞—á–µ–Ω–∏–µ - –∫–æ–ª-–≤–æ –≤ –≤—ã–≥—Ä—É–∑–∫–µ\n",
    "    \n",
    "    column - —Å—Ç–æ–ª–±–µ—Ü –≤ df, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏\n",
    "    to_df - bool, –µ—Å–ª–∏\n",
    "                    True - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç df\n",
    "                    False - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å\n",
    "    \"\"\"\n",
    "    dct = {}\n",
    "    for skill_list in column:\n",
    "        for skill in skill_list:\n",
    "            if skill not in dct.keys():\n",
    "                dct[skill] = 1\n",
    "            elif skill in dct.keys():\n",
    "                dct[skill] += 1\n",
    "    if to_df:\n",
    "        df_skills = pd.DataFrame.from_dict(dct, orient='index')\\\n",
    "            .rename(columns={0:'num_skills'})\\\n",
    "            .sort_values('num_skills', ascending=False)\n",
    "        return df_skills\n",
    "    else:\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4a235c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_df(df, google_sheet, create_sheet=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∂–∞–µ—Ç df –≤ Google Spreadsheets\n",
    "    \n",
    "    df - DataFrame\n",
    "    google_sheet - str, –Ω–∞–≤–∑–∞–Ω–∏–µ –ª–∏—Å—Ç–∞\n",
    "    create_sheet - bool, –µ—Å–ª–∏\n",
    "                    True - —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π –ª–∏—Å—Ç\n",
    "                    False - –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ç–µ–∫—É—â–∏–π –ª–∏—Å—Ç\n",
    "    \"\"\"\n",
    "    \n",
    "    df.key_skills = df.key_skills.astype('str')\n",
    "    df.description = df.description.str.replace(u'\\xa0', u' ').str.strip().str.replace(u'\\n', u' ')\n",
    "    \n",
    "    if create_sheet:\n",
    "        ws = work_table.add_worksheet(title=google_sheet, rows=100, cols=20)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    worksheet = work_table.worksheet(google_sheet)\n",
    "    worksheet.append_row(df.columns.tolist(), value_input_option='USER_ENTERED')\n",
    "    worksheet.append_rows(df.values.tolist(), value_input_option='USER_ENTERED')\n",
    "    \n",
    "    return print (f'–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ –ª–∏—Å—Ç {google_sheet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b3846a",
   "metadata": {},
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –≤—ã–≥—Ä—É–∑–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e801d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  –°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫  Created \n",
      "Directory  –ë–∏–∑–Ω–µ—Å - –∞–Ω–∞–ª–∏—Ç–∏–∫  Created \n",
      "Directory  –ê–Ω–∞–ª–∏—Ç–∏–∫ 1–°  Created \n",
      "Directory  –ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö  Created \n",
      "Directory  –ê–Ω–∞–ª–∏—Ç–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏  Created \n",
      "Directory  –ú–∞—Ä–∫–µ—Ç–æ–ª–æ–≥-–∞–Ω–∞–ª–∏—Ç–∏–∫  Created \n",
      "Directory  –ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫  Created \n",
      "Directory  Data scientist  Created \n",
      "Directory  BI –∞–Ω–∞–ª–∏—Ç–∏–∫  Created \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1157/1157 [16:13<00:00,  1.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 879/879 [12:17<00:00,  1.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 424/424 [05:56<00:00,  1.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 259/259 [03:33<00:00,  1.21it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:40<00:00,  1.17it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 137/137 [01:51<00:00,  1.23it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 163/163 [02:13<00:00,  1.22it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [02:40<00:00,  1.24it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [01:08<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫\n",
    "worksheet = work_table.worksheet('–°—Å—ã–ª–∫–∏')\n",
    "val = worksheet.get_values('B204:C213')\n",
    "columns = val.pop(0)\n",
    "val = pd.DataFrame(val, columns=columns)\n",
    "\n",
    "# —Å–æ–∑–¥–∞–µ–º –∏–∑ –Ω–∏—Ö —Å–ª–æ–≤–∞—Ä—å\n",
    "g_links_2 = dict(zip(val['–Ω–∞–∑–≤–∞–Ω–∏–µ'].values, val['—Å—Å—ã–ª–∫–∏'].values))\n",
    "\n",
    "# –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏\n",
    "add_folder = '–∞–Ω–∞–ª–∏—Ç–∏–∫–∞'\n",
    "\n",
    "# —Å–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü, –µ—Å–ª–∏ —Ç–∞–∫–∏—Ö –ø–∞–ø–æ–∫ –µ—â–µ –Ω–µ –±—ã–ª–æ\n",
    "for new_folder in val['–Ω–∞–∑–≤–∞–Ω–∏–µ'].values:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir(f'C:/Users/{user}/hh/{add_folder}/{new_folder}')\n",
    "        print(\"Directory \" , new_folder ,  \" Created \") \n",
    "    except FileExistsError:\n",
    "        print(\"Directory \" , new_folder ,  \" already exists\")\n",
    "        \n",
    "# –ø—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –∫–ª—é—á–∞–º —Å–ª–æ–≤–∞—Ä—è (–ø–∞–ø–∫–∞ - url) –∏ –≤—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é\n",
    "for key in g_links_2:\n",
    "    get_links(g_links_2[key], key, add_folder=add_folder, user=user)\n",
    "    \n",
    "# —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å, –≤ –∫–æ—Ç–æ—Ä–æ–º –∫–ª—é—á - –ø–∞–ø–∫–∞, –∞ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∫–ª—é—á—É\n",
    "dict_with_links_list_g_2 = {}\n",
    "for key in g_links_2:\n",
    "    dict_with_links_list_g_2[key] = get_list_of_vacancy(key, add_folder=add_folder, user=user)\n",
    "    \n",
    "all_df = get_vacancy_info(dict_with_links_list_g_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f675597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df = pd.DataFrame(columns=['name', 'salary_from', 'salary_to', 'salary_currency', 'salary_gross',\n",
    "       'experience_name', 'schedule_name', 'employment_name', 'description',\n",
    "       'key_skills', 'employer', 'published_at', 'url', 'views', 'type'])\n",
    "for key in all_df:\n",
    "    best_df = pd.concat([best_df, all_df[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa5564bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = pd.DataFrame()\n",
    "for key in all_df:\n",
    "    t = count_skills(all_df[key].key_skills).assign(type = key)\n",
    "    skills = pd.concat([t, skills])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d7e10c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_skills</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SQL</th>\n",
       "      <td>54</td>\n",
       "      <td>BI –∞–Ω–∞–ª–∏—Ç–∏–∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>30</td>\n",
       "      <td>BI –∞–Ω–∞–ª–∏—Ç–∏–∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power BI</th>\n",
       "      <td>28</td>\n",
       "      <td>BI –∞–Ω–∞–ª–∏—Ç–∏–∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Python</th>\n",
       "      <td>25</td>\n",
       "      <td>BI –∞–Ω–∞–ª–∏—Ç–∏–∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MS SQL</th>\n",
       "      <td>20</td>\n",
       "      <td>BI –∞–Ω–∞–ª–∏—Ç–∏–∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>–ö–∞–∑–Ω–∞—á–µ–π—Å—Ç–≤–æ</th>\n",
       "      <td>1</td>\n",
       "      <td>–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Ä—ã–Ω–æ–∫</th>\n",
       "      <td>1</td>\n",
       "      <td>–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>–î–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å</th>\n",
       "      <td>1</td>\n",
       "      <td>–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è</th>\n",
       "      <td>1</td>\n",
       "      <td>–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>–ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö</th>\n",
       "      <td>1</td>\n",
       "      <td>–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3444 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            num_skills                type\n",
       "SQL                                 54         BI –∞–Ω–∞–ª–∏—Ç–∏–∫\n",
       "l                                   30         BI –∞–Ω–∞–ª–∏—Ç–∏–∫\n",
       "Power BI                            28         BI –∞–Ω–∞–ª–∏—Ç–∏–∫\n",
       "Python                              25         BI –∞–Ω–∞–ª–∏—Ç–∏–∫\n",
       "MS SQL                              20         BI –∞–Ω–∞–ª–∏—Ç–∏–∫\n",
       "...                                ...                 ...\n",
       "–ö–∞–∑–Ω–∞—á–µ–π—Å—Ç–≤–æ                         1  –°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫\n",
       "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Ä—ã–Ω–æ–∫                     1  –°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫\n",
       "–î–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å            1  –°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫\n",
       "–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è           1  –°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫\n",
       "–ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö                        1  –°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫\n",
       "\n",
       "[3444 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4bc129b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ –ª–∏—Å—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫–∞\n"
     ]
    }
   ],
   "source": [
    "upload_df(best_df, '–∞–Ω–∞–ª–∏—Ç–∏–∫–∞', create_sheet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "548e5705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1DawPp9-eSygPpFe95pgOtVLF4x6lUaimtaH0ykg0DDA',\n",
       " 'tableRange': \"'–∞–Ω–∞–ª–∏—Ç–∏–∫–∞_skills'!A1:C1\",\n",
       " 'updates': {'spreadsheetId': '1DawPp9-eSygPpFe95pgOtVLF4x6lUaimtaH0ykg0DDA',\n",
       "  'updatedRange': \"'–∞–Ω–∞–ª–∏—Ç–∏–∫–∞_skills'!A2:C3445\",\n",
       "  'updatedRows': 3444,\n",
       "  'updatedColumns': 3,\n",
       "  'updatedCells': 10332}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = work_table.add_worksheet(title='–∞–Ω–∞–ª–∏—Ç–∏–∫–∞_skills', rows=100, cols=20)\n",
    "worksheet = work_table.worksheet('–∞–Ω–∞–ª–∏—Ç–∏–∫–∞_skills')\n",
    "worksheet.append_row(skills.reset_index().columns.tolist(), value_input_option='USER_ENTERED')\n",
    "worksheet.append_rows(skills.reset_index().values.tolist(), value_input_option='USER_ENTERED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f04b224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd92c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from airflow.decorators import dag, task\n",
    "import pandahouse as ph\n",
    "import telegram\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "\n",
    "connection = {'host': 'https://clickhouse.lab.karpov.courses',\n",
    "                      'database':'simulator_20221120',\n",
    "                      'user':'student', \n",
    "                      'password':'dpo_python_2020'\n",
    "                     }\n",
    "\n",
    "from matplotlib import style\n",
    "sns.set_theme(({**style.library[\"fivethirtyeight\"]}))\n",
    "plt.rcParams[\"figure.figsize\"] = (15,8)\n",
    "\n",
    "\n",
    "my_token = '5831544767:AAE-9VA_reObxmIb_FDZYeh9N4TiCslx-yc' \n",
    "bot = telegram.Bot(token=my_token) \n",
    "\n",
    "chat_id = -817095409\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'd-merinov-24',\n",
    "    'depends_on_past': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'start_date': datetime(2022, 12, 16),\n",
    "    'schedule_interval': '0 11 * * *'\n",
    "    }\n",
    "\n",
    "@dag(default_args=default_args, catchup=False)\n",
    "def lesson_7_dag_2_merinov():\n",
    "\n",
    "    @task()\n",
    "    def get_dau_df_2():\n",
    "        query = '''\n",
    "                    SELECT COUNT (DISTINCT user_id ) as uniq_users,\n",
    "                    day, os, gender, age, source \n",
    "                    FROM (\n",
    "\n",
    "                            SELECT user_id,\n",
    "                              toStartOfDay(toDateTime(time)) AS  day, os, gender, age, source \n",
    "                            FROM simulator_20221120.feed_actions \n",
    "                            GROUP BY user_id, day,os, gender, age, source \n",
    "                            HAVING day > (today()-1) - 7 and day != today()\n",
    "\n",
    "                            UNION ALL\n",
    "\n",
    "                            SELECT user_id,\n",
    "                              toStartOfDay(toDateTime(time)) AS  day, os, gender, age, source \n",
    "                            FROM simulator_20221120.message_actions \n",
    "                            GROUP BY user_id, day, os, gender, age, source \n",
    "                            HAVING day > (today()-1) - 7 and day != today()\n",
    "                              )\n",
    "                    GROUP BY day, os, gender, age, source \n",
    "                '''\n",
    "        dau_df = ph.read_clickhouse(query=query, connection=connection)\n",
    "        dau_df.day = dau_df.day.dt.date\n",
    "        return dau_df\n",
    "\n",
    "    @task()\n",
    "    def get_dau_info(dau_df):\n",
    "        dau = dau_df.groupby('day', as_index=False).agg({'uniq_users':'sum'})\n",
    "        dau['growth_rate'] = dau.uniq_users.pct_change()\n",
    "\n",
    "        date = dau.day.max()\n",
    "        dau_value = dau.query('day == @dau_df.day.max()').iloc[0][1]\n",
    "        diff = round((dau.query('day == @dau_df.day.max()').iloc[0][2]*100), 2)\n",
    "\n",
    "        if diff > 0:\n",
    "            change = '–±–æ–ª—å—à–µ'\n",
    "        else:\n",
    "            change = '–º–µ–Ω—å—à–µ'\n",
    "\n",
    "        title = 'üë•<b>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏</b>'\n",
    "\n",
    "        text_1 = f'–ó–∞ {date} DAU —Å–æ—Å—Ç–∞–≤–∏–ª {dau_value}, —á—Ç–æ –Ω–∞ {abs(diff)}% {change}, —á–µ–º –¥–Ω–µ–º —Ä–∞–Ω–µ–µ.'\n",
    "\n",
    "        source = dau_df.groupby(['day', 'source'], as_index=False)\\\n",
    "                    .agg({'uniq_users':'sum'})\\\n",
    "                    .sort_values('day')\n",
    "        source['ads_growth_rate'] = source.query('source == \"ads\"').uniq_users.pct_change()\n",
    "        source['organic_growth_rate'] = source.query('source == \"organic\"').uniq_users.pct_change()\n",
    "\n",
    "        ads_users = source.query('day == @dau_df.day.max() and source == \"ads\"').iloc[0][2]\n",
    "        organic_users = source.query('day == @dau_df.day.max() and source == \"organic\"').iloc[0][2]\n",
    "\n",
    "        ads_growth = round(source.query('day == @dau_df.day.max() and source == \"ads\"').iloc[0][3] * 100, 2)\n",
    "        organic_growth = round(source.query('day == @dau_df.day.max() and source == \"organic\"').iloc[0][4] * 100, 2)\n",
    "\n",
    "        text_2 = f'–ò—Ö –Ω–∏—Ö {ads_users} ({ads_growth}% –∫ –ø—Ä–µ–¥. –¥–Ω—é) –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —Ä–µ–∫–ª–∞–º—ã –∏ {organic_users} ({organic_growth}% –∫ –ø—Ä–µ–¥. –¥–Ω—é) —Å –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞. '\n",
    "\n",
    "        os = dau_df.groupby(['day', 'os'], as_index=False)\\\n",
    "                    .agg({'uniq_users':'sum'})\\\n",
    "                    .sort_values('day')\n",
    "        os['androind_growth_rate'] = os.query('os == \"Android\"').uniq_users.pct_change()\n",
    "        os['iOS_growth_rate'] = os.query('os == \"iOS\"').uniq_users.pct_change()\n",
    "\n",
    "        android_users = os.query('day == @dau_df.day.max() and os == \"Android\"').iloc[0][2]\n",
    "        ios_users = os.query('day == @dau_df.day.max() and os == \"iOS\"').iloc[0][2]\n",
    "\n",
    "        android_growth = round(os.query('day == @dau_df.day.max() and os == \"Android\"').iloc[0][3] * 100, 2)\n",
    "        ios_growth = round(os.query('day == @dau_df.day.max() and os == \"iOS\"').iloc[0][4] * 100, 2)\n",
    "\n",
    "        text_3 = f'–õ–µ–Ω—Ç–æ–π –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å {android_users} ({android_growth}% –∫ –ø—Ä–µ–¥. –¥–Ω—é) –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å Android –∏ {ios_users} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å iOS({ios_growth}%  –∫ –ø—Ä–µ–¥. –¥–Ω—é). '\n",
    "\n",
    "        return title + '\\n' + '\\n' + text_1 + '\\n' + text_2 + '\\n' + text_3 + '\\n'\n",
    "\n",
    "    @task()\n",
    "    def get_df_new_users():\n",
    "        query = '''with mess as (Select    user_id,\n",
    "                               min(toDate(time)) as bd,\n",
    "                           os, gender, age, source\n",
    "                            From simulator_20221120.message_actions \n",
    "                            Group by user_id, os, gender, age, source\n",
    "                            having bd > (today()-1) - 7 and bd != today()),\n",
    "                    feed as \n",
    "                                (Select    user_id,\n",
    "                                           min(toDate(time)) as bd,\n",
    "                                           os, gender, age, source\n",
    "                                From simulator_20221120.feed_actions \n",
    "                                Group by user_id, os, gender, age, source\n",
    "                                having bd > (today()-1) - 7 and bd != today())\n",
    "\n",
    "            select count(distinct user_id) as users, bd, os,gender, age, source from feed l\n",
    "            full Join mess r on l.user_id = r.user_id \n",
    "                        AND l.bd=r.bd \n",
    "                        AND l.os=r.os \n",
    "                        AND l.gender=r.gender \n",
    "                        AND l.age=r.age \n",
    "                        AND l.source = r.source\n",
    "            group by bd, os,gender, age, source\n",
    "            ORDER BY bd DESC'''\n",
    "        df_new_users = ph.read_clickhouse(query=query, connection=connection)\n",
    "        df_new_users.bd = df_new_users.bd.dt.date\n",
    "        return df_new_users\n",
    "    \n",
    "    @task()\n",
    "    def get_info_new_users(df_new_users):\n",
    "\n",
    "        new_users = df_new_users.groupby('bd', as_index=False).agg({'users':'sum'})\n",
    "        new_users['growth_rate'] = new_users.users.pct_change()\n",
    "\n",
    "        date = new_users.bd.max()\n",
    "        new_users_value = new_users.query('bd == @df_new_users.bd.max()').iloc[0][1]\n",
    "        diff = round((new_users.query('bd == @df_new_users.bd.max()').iloc[0][2]*100), 2)\n",
    "\n",
    "        if diff > 0:\n",
    "            change = '–±–æ–ª—å—à–µ'\n",
    "        else:\n",
    "            change = '–º–µ–Ω—å—à–µ'\n",
    "\n",
    "        title = \"üÜï<b>–ù–æ–≤—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏</b>\"\n",
    "\n",
    "        text_1 = f'–ó–∞ –¥–µ–Ω—å –ø—Ä–∏—à–ª–æ {new_users_value} –Ω–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —á—Ç–æ –Ω–∞ {abs(diff)}% {change}, —á–µ–º –¥–Ω–µ–º —Ä–∞–Ω–µ–µ.'\n",
    "\n",
    "        source = df_new_users.groupby(['bd', 'source'], as_index=False)\\\n",
    "                    .agg({'users':'sum'})\\\n",
    "                    .sort_values('bd')\n",
    "\n",
    "        source['ads_growth_rate'] = source.query('source == \"ads\"').users.pct_change()\n",
    "        source['organic_growth_rate'] = source.query('source == \"organic\"').users.pct_change()\n",
    "\n",
    "        ads_users = source.query('bd == @df_new_users.bd.max() and source == \"ads\"').iloc[0][2]\n",
    "        organic_users = source.query('bd == @df_new_users.bd.max() and source == \"organic\"').iloc[0][2]\n",
    "\n",
    "        ads_growth = round(source.query('bd == @df_new_users.bd.max() and source == \"ads\"').iloc[0][3] * 100, 2)\n",
    "        organic_growth = round(source.query('bd == @df_new_users.bd.max() and source == \"organic\"').iloc[0][4] * 100, 2)\n",
    "\n",
    "        text_2 = f'–ò—Ö –Ω–∏—Ö {ads_users} ({ads_growth}% –∫ –ø—Ä–µ–¥. –¥–Ω—é) –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —Ä–µ–∫–ª–∞–º—ã –∏ {organic_users} ({organic_growth}% –∫ –ø—Ä–µ–¥. –¥–Ω—é) —Å –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞. '\n",
    "\n",
    "        df_new_users['age_cut'] = pd.cut(df_new_users.age, [0, 15, 21, 27, 35, 45, 60, 70, 150])\n",
    "\n",
    "        male = df_new_users.groupby(['gender', 'bd'])['users'].sum()\\\n",
    "                        .to_frame().reset_index()\\\n",
    "                        .query('bd == @df_new_users.bd.max() and gender == 1')\\\n",
    "                        .iloc[0][2]\n",
    "        female = df_new_users.groupby(['gender', 'bd'])['users'].sum()\\\n",
    "                        .to_frame().reset_index()\\\n",
    "                        .query('bd == @df_new_users.bd.max() and gender == 0')\\\n",
    "                        .iloc[0][2]\n",
    "\n",
    "        age = df_new_users.groupby(['age_cut', 'bd'])['users'].sum()\\\n",
    "                        .to_frame().reset_index()\\\n",
    "                        .sort_values(['bd', 'users'], ascending=False)\\\n",
    "                        .iloc[0][0]\n",
    "        male_share = round(male/(female+male)*100)\n",
    "        female_share = round(female/(female+male)*100)\n",
    "\n",
    "        text_3 = f'–°—Ä–µ–¥–∏ –Ω–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –º—É–∂—á–∏–Ω - {male} ({male_share}%) —á–µ–ª–æ–≤–µ–∫, –¥–µ–≤—É—à–µ–∫ - {female} ({female_share}%) —á–µ–ª–æ–≤–µ–∫.  –ù–∞–∏–±–æ–ª—å—à–µ–µ —á–∏—Å–ª–æ –Ω–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ {age}'\n",
    "\n",
    "\n",
    "        return title+ '\\n' + '\\n' + text_1 + '\\n' + text_2 + '\\n' + text_3\n",
    "    \n",
    "    @task()\n",
    "    def get_likes_views_df():\n",
    "            query = '''SELECT toStartOfDay(toDateTime(time)) AS day,\n",
    "                           count(user_id) as actions,\n",
    "                           action \n",
    "                      FROM simulator_20221120.feed_actions\n",
    "                      WHERE day > (today()-1) - 7 and day != today()\n",
    "                      GROUP BY toStartOfDay(toDateTime(time)), action\n",
    "                      ORDER BY day DESC '''\n",
    "\n",
    "            likes_views_df = ph.read_clickhouse(query=query, connection=connection)\n",
    "            likes_views_df.day = likes_views_df.day.dt.date\n",
    "            return likes_views_df\n",
    "\n",
    "    @task()\n",
    "    def get_messages_df():\n",
    "            query = '''SELECT toStartOfDay(toDateTime(time)) AS day,\n",
    "                           count(user_id) as messages\n",
    "                    FROM simulator_20221120.message_actions\n",
    "                    WHERE day > (today()-1) - 7 and day != today()\n",
    "                    GROUP BY toStartOfDay(toDateTime(time))\n",
    "                    ORDER BY day DESC'''\n",
    "            messages_df = ph.read_clickhouse(query=query, connection=connection)\n",
    "            # messages_df.day = likes_views_df.day.dt.date\n",
    "\n",
    "            return messages_df\n",
    "    @task()\n",
    "    def get_info_likes_views_mess(likes_views_df, messages_df):\n",
    "\n",
    "\n",
    "        actions_df = likes_views_df.groupby(['day', 'action'], as_index=False)\\\n",
    "                    .agg({'actions':'sum'})\\\n",
    "                    .sort_values('day')\n",
    "        actions_df['like_growth_rate'] = actions_df.query('action == \"like\"').actions.pct_change()\n",
    "        actions_df['view_growth_rate'] = actions_df.query('action == \"view\"').actions.pct_change()\n",
    "\n",
    "        likes = actions_df.query('day == @actions_df.day.max() and action == \"like\"').iloc[0][2]\n",
    "        views = actions_df.query('day == @actions_df.day.max() and action == \"view\"').iloc[0][2]\n",
    "\n",
    "        likes_growth = round(actions_df.query('day == @actions_df.day.max() and action == \"like\"').iloc[0][3] * 100, 2)\n",
    "        views_growth = round(actions_df.query('day == @actions_df.day.max() and action == \"view\"').iloc[0][4] * 100, 2)\n",
    "\n",
    "        ctr = round(likes/views, 4)*100\n",
    "\n",
    "        title = \"üíñüí¨<b>–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</b>\"\n",
    "\n",
    "        text_1 = f'–ó–∞ –≤—á–µ—Ä–∞ –±—ã–ª–æ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ {likes} –ª–∞–π–∫–æ–≤ ({likes_growth}% –∫ –ø—Ä–µ–¥. –¥–Ω—é) –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ {views} –ø–æ—Å—Ç–æ–≤ ({views_growth}% –∫ –ø—Ä–µ–¥. –¥–Ω—é). CTR —Å–æ—Å—Ç–∞–≤–∏–ª  {ctr}%'\n",
    "\n",
    "        mes_1 = messages_df.sort_values('day', ascending=False).iloc[0][1]\n",
    "        mes_0 = messages_df.sort_values('day', ascending=False).iloc[1][1]\n",
    "\n",
    "        mes_diff = round(mes_1/mes_0 - 1, 2)*100\n",
    "\n",
    "        text_2 = f'–¢–∞–∫–∂–µ –±—ã–ª–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {mes_1} —Å–æ–æ–±—â–µ–Ω–∏–π ({mes_diff}% –∫ –ø—Ä–µ–¥. –¥–Ω—é)'\n",
    "\n",
    "        return title + '\\n'+ '\\n' + text_1 + '\\n' + text_2\n",
    "    \n",
    "    @task()\n",
    "    def send_plot_dau_df(dau_df):\n",
    "        dau_df.day = pd.to_datetime(dau_df[\"day\"])\n",
    "        dau_df = dau_df.sort_values('day')\n",
    "        dau_df.day = dau_df.day.dt.strftime('%d-%m')\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.title('–î–∏–Ω–∞–º–∏–∫–∞ DAU')\n",
    "        sns.lineplot(y = 'uniq_users', x='day', data=dau_df)\n",
    "        plt.subplot(221)\n",
    "        plt.title('–î–∏–Ω–∞–º–∏–∫–∞ DAU –≤ —Ä–∞–∑–±–∏–≤–∫–µ –ø–æ OS')\n",
    "        sns.lineplot(y = dau_df.uniq_users, x=dau_df.day, hue=dau_df.os)\n",
    "        plt.subplot(222)\n",
    "        plt.title('–î–∏–Ω–∞–º–∏–∫–∞ DAU –≤ —Ä–∞–∑–±–∏–≤–∫–µ –ø–æ Source')\n",
    "        sns.lineplot(y = dau_df.uniq_users, x=dau_df.day, hue=dau_df.source)\n",
    "\n",
    "        plot_object = io.BytesIO()\n",
    "        plt.savefig(plot_object)\n",
    "        plot_object.seek(0)\n",
    "        plot_object.name = 'dau.png'\n",
    "        plt.close()\n",
    "        bot.sendPhoto(chat_id=chat_id, photo=plot_object, parse_mode='HTML')\n",
    "        \n",
    "    @task()\n",
    "    def send_plot_new_users_df(df_new_users):\n",
    "        df_new_users.bd = pd.to_datetime(df_new_users.bd)\n",
    "        df_new_users = df_new_users.sort_values('bd').query('bd > \"1971-01-01\"')\n",
    "        df_new_users.bd = df_new_users.bd.dt.strftime('%d-%m')\n",
    "\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.title('–¥–∏–Ω–∞–º–∏–∫–∞ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π')\n",
    "        sns.lineplot(y = 'users', x='bd', data=df_new_users)\n",
    "        plt.subplot(221)\n",
    "        plt.title('–ù–æ–≤—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤ —Ä–∞–∑—Ä–µ–∑–µ OS')\n",
    "        sns.lineplot(y = 'users', x='bd', hue='os', data=df_new_users)\n",
    "        plt.subplot(222)\n",
    "        plt.title('–ù–æ–≤—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤ —Ä–∞–∑—Ä–µ–∑–µ Source')\n",
    "        sns.lineplot(y = 'users', x='bd', hue='source', data=df_new_users)\n",
    "\n",
    "        plot_object = io.BytesIO()\n",
    "        plt.savefig(plot_object)\n",
    "        plot_object.seek(0)\n",
    "        plot_object.name = 'dau.png'\n",
    "        plt.close()\n",
    "        bot.sendPhoto(chat_id=chat_id, photo=plot_object, parse_mode='HTML')\n",
    "        \n",
    "    @task()\n",
    "    def send_plot_likes_views_df(likes_views_df, messages_df):\n",
    "        likes_views_df.day = pd.to_datetime(likes_views_df[\"day\"])\n",
    "        likes_views_df = likes_views_df.sort_values('day')\n",
    "        likes_views_df.day = likes_views_df.day.dt.strftime('%d-%m')\n",
    "\n",
    "        messages_df.day = pd.to_datetime(messages_df[\"day\"])\n",
    "        messages_df = messages_df.sort_values('day')\n",
    "        messages_df.day = messages_df.day.dt.strftime('%d-%m')\n",
    "\n",
    "        ctr = likes_views_df.pivot_table(index='day', columns='action', values='actions').reset_index()\n",
    "        ctr['ctr'] = ctr.like / ctr.view\n",
    "        ctr['ctr'] = ctr.ctr.mul(100).round(2)\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.title('–î–∏–Ω–∞–º–∏–∫–∞ CTR, %')\n",
    "        sns.lineplot(y = 'ctr', x='day', data=ctr)\n",
    "        plt.subplot(221)\n",
    "        plt.title('–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –ª–µ–Ω—Ç–µ')\n",
    "        sns.lineplot(y = 'actions', x='day', hue='action', data=likes_views_df)\n",
    "        plt.subplot(222)\n",
    "        plt.title('–ö–æ–ª-–≤–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π')\n",
    "        sns.lineplot(y = 'messages', x='day', data=messages_df)\n",
    "\n",
    "        plot_object = io.BytesIO()\n",
    "        plt.savefig(plot_object)\n",
    "        plot_object.seek(0)\n",
    "        plot_object.name = 'dau.png'\n",
    "        plt.close()\n",
    "        bot.sendPhoto(chat_id=chat_id, photo=plot_object, parse_mode='HTML')\n",
    "        \n",
    "    @task()\n",
    "    def send_message(title):\n",
    "        bot.sendMessage(chat_id=chat_id, text=title, parse_mode='HTML')\n",
    "        \n",
    "    @task()\n",
    "    def send_message_title():\n",
    "        ds = context['ds']\n",
    "        bot.sendMessage(chat_id=chat_id, text=f\"üìÑ<b>–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –ª–µ–Ω—Ç–µ –Ω–æ–≤–æ—Å—Ç–µ–π, –∏ –ø–æ —Å–µ—Ä–≤–∏—Å—É –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π. –î–∞—Ç–∞: {ds}</b>\", parse_mode='HTML')\n",
    "    \n",
    "    send_message_title()\n",
    "    dau_df = get_dau_df_2()\n",
    "    title_1 = get_dau_info(dau_df)\n",
    "    send_message(title_1)\n",
    "    send_plot_dau_df(dau_df)\n",
    "    df_new_users = get_df_new_users()\n",
    "    title_2 = get_info_new_users(df_new_users)\n",
    "    send_message(title_2)\n",
    "    send_plot_new_users_df(df_new_users)\n",
    "    likes_views_df = get_likes_views_df()\n",
    "    messages_df = get_messages_df()\n",
    "    title_3 = get_info_likes_views_mess(likes_views_df, messages_df)\n",
    "    send_message(title_3)\n",
    "    send_plot_likes_views_df(likes_views_df, messages_df)\n",
    "    \n",
    "lesson_7_dag_2_merinov = lesson_7_dag_2_merinov()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
